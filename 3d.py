import os
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d.art3d import Poly3DCollection
import torch
import torch.nn as nn
import torch.optim as optim
from scipy.spatial import ConvexHull, cKDTree
from scipy.ndimage import gaussian_filter
from skimage import measure, morphology
from collections import deque
from tqdm import tqdm
import json
import warnings
import plotly.graph_objects as go

# è®¾ç½®æ”¯æŒä¸­æ–‡çš„å­—ä½“ï¼ˆä¼˜å…ˆä½¿ç”¨å¸¸è§çš„ä¸­æ–‡å­—ä½“ï¼‰
plt.rcParams['font.sans-serif'] = ['Microsoft YaHei', 'SimHei', 'Arial Unicode MS', 'DejaVu Sans', 'Lucida Grande']
plt.rcParams['axes.unicode_minus'] = False  # è§£å†³è´Ÿå·'-'æ˜¾ç¤ºä¸ºæ–¹å—çš„é—®é¢˜

warnings.filterwarnings('ignore')


# ===================== 1. ç†è®ºåˆ†æï¼šä½“ç´ ç©ºé—´ä¸é¢—ç²’æ•°é‡å…³ç³» =====================
class VoxelCapacityAnalyzer:
    """åˆ†æä¸åŒä½“ç´ ç©ºé—´ä¸‹å¯å®¹çº³çš„é¢—ç²’æ•°é‡"""

    @staticmethod
    def analyze_capacity(voxel_sizes=[256, 512, 1024, 2048]):
        """
        åˆ†æä¸åŒä½“ç´ ç©ºé—´çš„å®¹çº³èƒ½åŠ›

        Parameters:
        -----------
        voxel_sizes : list
            ä½“ç´ ç©ºé—´è¾¹é•¿åˆ—è¡¨

        Returns:
        --------
        dict : åˆ†æç»“æœ
        """
        results = {}

        print("=" * 80)
        print("ä½“ç´ ç©ºé—´ä¸é¢—ç²’å®¹çº³èƒ½åŠ›åˆ†æ")
        print("=" * 80)

        for size in voxel_sizes:
            total_voxels = size ** 3

            # åˆ†æä¸åŒå¤æ‚åº¦é¢—ç²’
            analysis = {
                'voxel_size': size,
                'total_voxels': total_voxels,
                'particle_types': {}
            }

            # æœ€å°å¯åˆ†è¾¨é¢—ç²’ï¼š5ä½“ç´ ç›´å¾„ï¼ˆè¾¹ç¼˜æ¨¡ç³Šå ç”¨ï¼‰
            min_diameter = 5

            # è€ƒè™‘è¾¹ç¼˜æ¨¡ç³Šï¼šå®é™…å ç”¨ = é¢—ç²’ç›´å¾„ + 2*æ¨¡ç³ŠåŒºåŸŸ(~10ä½“ç´ )
            blur_margin = 10

            particle_configs = [
                ('ç®€å•å¤šé¢ä½“(4-8é¢)', min_diameter, 1.0),
                ('ä¸­ç­‰å¤šé¢ä½“(9-20é¢)', min_diameter * 1.5, 1.2),
                ('å¤æ‚å¤šé¢ä½“(21-50é¢)', min_diameter * 2.0, 1.4),
                ('é«˜å¤æ‚å¤šé¢ä½“(51-100é¢)', min_diameter * 2.5, 1.6),
                ('è¿‘ä¼¼æ¤­çƒä½“', min_diameter * 3.0, 1.8)
            ]

            for ptype, base_diameter, complexity_factor in particle_configs:
                # å®é™…å ç”¨ç›´å¾„
                effective_diameter = base_diameter * complexity_factor + blur_margin
                effective_radius = effective_diameter / 2

                # å•é¢—ç²’å ç”¨ä½“ç§¯ï¼ˆè€ƒè™‘çƒå½¢åŒ…ç»œï¼‰
                particle_volume = (4 / 3) * np.pi * (effective_radius ** 3)

                # éšæœºå †ç§¯æ•ˆç‡ï¼ˆçº¦64%ï¼‰
                packing_efficiency = 0.64

                # æœ€å¤§å®¹çº³æ•°é‡
                max_particles = int((total_voxels * packing_efficiency) / particle_volume)

                # è€ƒè™‘å­”éš™ç‡è¦æ±‚ï¼ˆ0.00001 - 0.0003ï¼‰
                # å›ºä½“ç‡ = 0.9997 - 0.99999
                for target_porosity in [0.00001, 0.0001, 0.0003]:
                    solid_fraction = 1 - target_porosity
                    actual_particles = int(max_particles * solid_fraction / packing_efficiency)

                    key = f"{ptype}_porosity_{target_porosity}"
                    analysis['particle_types'][key] = {
                        'particle_type': ptype,
                        'base_diameter': base_diameter,
                        'effective_diameter': effective_diameter,
                        'complexity_factor': complexity_factor,
                        'target_porosity': target_porosity,
                        'max_particles': actual_particles,
                        'particle_volume': particle_volume
                    }

            results[size] = analysis

            print(f"\nä½“ç´ ç©ºé—´: {size}Â³ = {total_voxels:,} ä½“ç´ ")
            print("-" * 80)
            print(f"{'é¢—ç²’ç±»å‹':<25} {'å­”éš™ç‡':<10} {'æœ€å¤§é¢—ç²’æ•°':<15} {'æœ‰æ•ˆç›´å¾„':<15}")
            print("-" * 80)

            for key, data in analysis['particle_types'].items():
                if '0.0003' in key:  # åªæ˜¾ç¤ºç›®æ ‡å­”éš™ç‡
                    print(f"{data['particle_type']:<25} {data['target_porosity']:<10.5f} "
                          f"{data['max_particles']:<15,} {data['effective_diameter']:<15.1f}")

        print("\n" + "=" * 80)
        print("å…³é”®ç»“è®ºï¼š")
        print("1. æœ€å°å¯åˆ†è¾¨é¢—ç²’ç›´å¾„ï¼š5ä½“ç´ ï¼ˆè€ƒè™‘è¾¹ç¼˜æ¨¡ç³Šï¼‰")
        print("2. è¾¹ç¼˜æ¨¡ç³ŠåŒºåŸŸï¼šå¹³å‡5ä½“ç´ ï¼Œæ–¹å·®2ä½“ç´ ï¼ˆæ­£æ€åˆ†å¸ƒï¼‰")
        print("3. é¢—ç²’å¤æ‚åº¦ä¸æ•°é‡å‘ˆåæ¯”å…³ç³»")
        print("4. æä½å­”éš™ç‡ä¸‹ï¼Œéœ€è¦å¤§é‡é«˜å¯†åº¦é¢—ç²’å †ç§¯")
        print("=" * 80 + "\n")

        return results


# ===================== 2. å¤šé¢ä½“ç”Ÿæˆå™¨ï¼ˆæ”¯æŒ4-100é¢ä½“ï¼‰ =====================
class PolyhedronGenerator:
    """ç”Ÿæˆå„ç§å¤æ‚åº¦çš„å¤šé¢ä½“"""

    @staticmethod
    def generate_polyhedron(n_faces, radius=1.0):
        """
        ç”Ÿæˆné¢ä½“

        Parameters:
        -----------
        n_faces : int
            é¢æ•°ï¼ˆ4-100ï¼‰
        radius : float
            å¤–æ¥çƒåŠå¾„

        Returns:
        --------
        vertices : ndarray
            é¡¶ç‚¹åæ ‡
        """
        if n_faces == 4:  # å››é¢ä½“
            vertices = np.array([
                [1, 1, 1], [1, -1, -1], [-1, 1, -1], [-1, -1, 1]
            ])
        elif n_faces == 5:  # äº”é¢ä½“ï¼ˆä¸‰è§’åŒé”¥ï¼‰
            vertices = np.array([
                [0, 0, 1], [0, 0, -1],
                [1, 0, 0], [np.cos(2 * np.pi / 3), np.sin(2 * np.pi / 3), 0],
                [np.cos(4 * np.pi / 3), np.sin(4 * np.pi / 3), 0]
            ])
        elif n_faces == 6:  # å…­é¢ä½“ï¼ˆç«‹æ–¹ä½“ï¼‰
            vertices = np.array([
                [1, 1, 1], [1, 1, -1], [1, -1, 1], [1, -1, -1],
                [-1, 1, 1], [-1, 1, -1], [-1, -1, 1], [-1, -1, -1]
            ])
        elif n_faces == 8:  # å…«é¢ä½“
            vertices = np.array([
                [1, 0, 0], [-1, 0, 0], [0, 1, 0],
                [0, -1, 0], [0, 0, 1], [0, 0, -1]
            ])
        elif n_faces == 12:  # åäºŒé¢ä½“
            phi = (1 + np.sqrt(5)) / 2
            vertices = np.array([
                [1, 1, 1], [1, 1, -1], [1, -1, 1], [1, -1, -1],
                [-1, 1, 1], [-1, 1, -1], [-1, -1, 1], [-1, -1, -1],
                [0, phi, 1 / phi], [0, phi, -1 / phi], [0, -phi, 1 / phi], [0, -phi, -1 / phi],
                [1 / phi, 0, phi], [1 / phi, 0, -phi], [-1 / phi, 0, phi], [-1 / phi, 0, -phi],
                [phi, 1 / phi, 0], [phi, -1 / phi, 0], [-phi, 1 / phi, 0], [-phi, -1 / phi, 0]
            ])
        elif n_faces == 20:  # äºŒåé¢ä½“
            phi = (1 + np.sqrt(5)) / 2
            vertices = np.array([
                [0, 1, phi], [0, 1, -phi], [0, -1, phi], [0, -1, -phi],
                [1, phi, 0], [1, -phi, 0], [-1, phi, 0], [-1, -phi, 0],
                [phi, 0, 1], [phi, 0, -1], [-phi, 0, 1], [-phi, 0, -1]
            ])
        else:
            # å¯¹äºå…¶ä»–é¢æ•°ï¼Œä½¿ç”¨çƒé¢å‡åŒ€åˆ†å¸ƒè¿‘ä¼¼
            n_points = max(n_faces // 2, 12)

            # Fibonacciçƒé¢é‡‡æ ·
            indices = np.arange(0, n_points, dtype=float) + 0.5
            phi = np.arccos(1 - 2 * indices / n_points)
            theta = np.pi * (1 + 5 ** 0.5) * indices

            vertices = np.column_stack([
                np.sin(phi) * np.cos(theta),
                np.sin(phi) * np.sin(theta),
                np.cos(phi)
            ])

        # å½’ä¸€åŒ–åˆ°æŒ‡å®šåŠå¾„
        vertices = vertices / np.linalg.norm(vertices, axis=1, keepdims=True) * radius

        return vertices


# ===================== 3. æ¨¡ç³Šè¾¹ç¼˜æ³•æ ¸å¿ƒç®—æ³• =====================
class BlurredEdgeMethod:
    """æ¨¡ç³Šè¾¹ç¼˜æ³•ç”Ÿæˆå­”éš™ç»“æ„"""

    def __init__(self, vol_size=256, target_porosity=0.0003,
                 particle_diameter=10, compactness=0.95,
                 blur_mean=5, blur_std=2, save_dir='output_blurred'):
        """
        åˆå§‹åŒ–

        Parameters:
        -----------
        vol_size : int
            ä½“ç´ ç©ºé—´è¾¹é•¿
        target_porosity : float
            ç›®æ ‡å­”éš™ç‡ï¼ˆ0.00001 - 0.0003ï¼‰
        particle_diameter : int
            é¢—ç²’ç­‰æ•ˆç›´å¾„ï¼ˆä½“ç´ ï¼‰
        compactness : float
            å¯†å®åº¦ï¼ˆ0-1ï¼‰
        blur_mean : float
            è¾¹ç¼˜æ¨¡ç³Šå¹³å‡å€¼ï¼ˆä½“ç´ ï¼‰
        blur_std : float
            è¾¹ç¼˜æ¨¡ç³Šæ ‡å‡†å·®ï¼ˆä½“ç´ ï¼‰
        """
        self.vol_size = vol_size
        self.target_porosity = target_porosity
        self.particle_diameter = particle_diameter
        self.compactness = compactness
        self.blur_mean = blur_mean
        self.blur_std = blur_std
        self.save_dir = save_dir

        os.makedirs(save_dir, exist_ok=True)

        # è®¡ç®—æ‰€éœ€é¢—ç²’æ•°
        self.n_particles = self._calculate_particle_number()

        self.particles = []
        self.volume = None
        self.gap_mask = None
        self.gap_regions = None
        self.widest_path = None

    def _calculate_particle_number(self):
        """è®¡ç®—è¾¾åˆ°ç›®æ ‡å­”éš™ç‡æ‰€éœ€çš„é¢—ç²’æ•°"""
        total_voxels = self.vol_size ** 3
        target_solid_voxels = total_voxels * (1 - self.target_porosity)

        # å•é¢—ç²’ä½“ç§¯ï¼ˆè€ƒè™‘å¯†å®åº¦ï¼‰
        radius = (self.particle_diameter / 2) * self.compactness
        particle_volume = (4 / 3) * np.pi * (radius ** 3)

        n_particles_theory = int(target_solid_voxels / particle_volume)

        # è€ƒè™‘ç©ºé—´é™åˆ¶ï¼šè®¡ç®—å®é™…å¯å®¹çº³çš„æœ€å¤§é¢—ç²’æ•°
        # æœ€å°é—´è·ï¼ˆåŒ…æ‹¬è¾¹ç¼˜æ¨¡ç³Šï¼‰
        min_separation = self.particle_diameter + self.blur_mean * 2

        # ä¼°ç®—å¯å®¹çº³çš„é¢—ç²’æ•°ï¼ˆç®€å•ç«‹æ–¹å †ç§¯ï¼‰
        particles_per_dimension = int(self.vol_size / min_separation)
        max_particles = particles_per_dimension ** 3

        # å–ç†è®ºå€¼å’Œå®é™…ç©ºé—´é™åˆ¶çš„è¾ƒå°å€¼
        n_particles = min(n_particles_theory, max_particles)

        print(f"ç›®æ ‡å­”éš™ç‡: {self.target_porosity:.5f}")
        print(f"ç†è®ºæ‰€éœ€é¢—ç²’æ•°: {n_particles_theory}")
        print(f"ç©ºé—´é™åˆ¶æœ€å¤§é¢—ç²’æ•°: {max_particles}")
        print(f"å®é™…ä½¿ç”¨é¢—ç²’æ•°: {n_particles}")

        # å¦‚æœå·®è·è¿‡å¤§ï¼Œç»™å‡ºè­¦å‘Š
        if n_particles < n_particles_theory * 0.5:
            print(f"âš ï¸  è­¦å‘Šï¼šç©ºé—´é™åˆ¶å¯¼è‡´é¢—ç²’æ•°ä¸è¶³ï¼Œå¯èƒ½æ— æ³•è¾¾åˆ°ç›®æ ‡å­”éš™ç‡")
            print(f"ğŸ’¡ å»ºè®®ï¼š")
            print(f"   1. å¢å¤§ä½“ç´ ç©ºé—´å°ºå¯¸ï¼ˆå½“å‰ï¼š{self.vol_size}ï¼‰")
            print(f"   2. å‡å°é¢—ç²’ç›´å¾„ï¼ˆå½“å‰ï¼š{self.particle_diameter}ï¼‰")
            print(f"   3. å‡å°è¾¹ç¼˜æ¨¡ç³Šå‚æ•°ï¼ˆå½“å‰ï¼š{self.blur_mean}Â±{self.blur_std}ï¼‰")

        return n_particles

    def generate_particles(self):
        """ç”Ÿæˆé¢—ç²’ï¼ˆä¸å…±äº«é¢å’Œè¾¹ï¼‰"""
        print(f"ç”Ÿæˆ {self.n_particles} ä¸ªé¢—ç²’...")

        # åŠ¨æ€è°ƒæ•´æœ€å°é—´è·ç­–ç•¥
        # å¯¹äºæä½å­”éš™ç‡ï¼Œéœ€è¦é¢—ç²’æ›´ç´§å¯†æ’åˆ—
        if self.target_porosity < 0.001:
            # æä½å­”éš™ç‡ï¼šé¢—ç²’éœ€è¦ç´§å¯†æ’åˆ—ï¼Œå‡å°é—´è·è¦æ±‚
            min_separation = self.particle_diameter * 0.8 + self.blur_mean
            print(f"æä½å­”éš™ç‡æ¨¡å¼ï¼šä½¿ç”¨è¾ƒå°é—´è· {min_separation:.2f} ä½“ç´ ")
        else:
            min_separation = self.particle_diameter * 1.2 + self.blur_mean * 2

        centers = []
        kdtree = None

        # å¢åŠ å°è¯•æ¬¡æ•°
        max_attempts = max(self.n_particles * 500, 50000)
        attempts = 0

        # éšæœºç”Ÿæˆé¢æ•°ï¼ˆ4-100é¢ä½“ï¼‰
        face_distribution = np.random.choice(
            list(range(4, 101)),
            size=self.n_particles * 2,  # ç”Ÿæˆæ›´å¤šå¤‡ç”¨
            p=self._get_face_probability_distribution()
        )

        pbar = tqdm(total=self.n_particles, desc="ç”Ÿæˆé¢—ç²’ä¸­å¿ƒ")

        # ä¼˜åŒ–ç­–ç•¥ï¼šå…ˆåœ¨ç½‘æ ¼ç‚¹é™„è¿‘ç”Ÿæˆï¼Œç¡®ä¿è¦†ç›–
        grid_spacing = min_separation * 1.1
        grid_size = int(self.vol_size / grid_spacing)

        # ç¬¬ä¸€é˜¶æ®µï¼šç½‘æ ¼åŒ–ç”Ÿæˆï¼ˆç¡®ä¿åŸºæœ¬è¦†ç›–ï¼‰
        margin = self.particle_diameter
        if grid_size >= 2:
            grid_points = []
            for i in range(grid_size):
                for j in range(grid_size):
                    for k in range(grid_size):
                        x = margin + i * grid_spacing + np.random.uniform(-grid_spacing * 0.2, grid_spacing * 0.2)
                        y = margin + j * grid_spacing + np.random.uniform(-grid_spacing * 0.2, grid_spacing * 0.2)
                        z = margin + k * grid_spacing + np.random.uniform(-grid_spacing * 0.2, grid_spacing * 0.2)

                        # ç¡®ä¿åœ¨è¾¹ç•Œå†…
                        x = np.clip(x, margin, self.vol_size - margin)
                        y = np.clip(y, margin, self.vol_size - margin)
                        z = np.clip(z, margin, self.vol_size - margin)

                        grid_points.append(np.array([x, y, z]))

            # éšæœºæ‰“ä¹±
            np.random.shuffle(grid_points)

            # ä½¿ç”¨ç½‘æ ¼ç‚¹
            for center in grid_points:
                if len(centers) >= self.n_particles:
                    break

                if kdtree is None:
                    centers.append(center)
                    kdtree = cKDTree(centers)
                    pbar.update(1)
                else:
                    dist, _ = kdtree.query(center)
                    if dist > min_separation * 0.9:  # ç¨å¾®æ”¾æ¾è¦æ±‚
                        centers.append(center)
                        kdtree = cKDTree(centers)
                        pbar.update(1)

        # ç¬¬äºŒé˜¶æ®µï¼šéšæœºå¡«å……å‰©ä½™ç©ºé—´
        while len(centers) < self.n_particles and attempts < max_attempts:
            # éšæœºç”Ÿæˆä¸­å¿ƒ
            margin = self.particle_diameter
            center = np.array([
                np.random.uniform(margin, self.vol_size - margin),
                np.random.uniform(margin, self.vol_size - margin),
                np.random.uniform(margin, self.vol_size - margin)
            ])

            # æ£€æŸ¥ä¸å·²æœ‰é¢—ç²’çš„è·ç¦»
            if kdtree is None:
                centers.append(center)
                kdtree = cKDTree(centers)
                pbar.update(1)
            else:
                dist, _ = kdtree.query(center)
                # åŠ¨æ€è°ƒæ•´æ¥å—æ ‡å‡†
                accept_threshold = min_separation * (1.0 - 0.3 * len(centers) / self.n_particles)
                if dist > accept_threshold:
                    centers.append(center)
                    kdtree = cKDTree(centers)
                    pbar.update(1)

            attempts += 1

        pbar.close()

        actual_generated = len(centers)
        if actual_generated < self.n_particles:
            print(f"âš ï¸  è­¦å‘Š: ä»…ç”Ÿæˆäº† {actual_generated} ä¸ªé¢—ç²’ï¼ˆç›®æ ‡{self.n_particles}ï¼‰")
            print(f"ç”Ÿæˆç‡: {100 * actual_generated / self.n_particles:.1f}%")
            self.n_particles = actual_generated
        else:
            print(f"âœ“ æˆåŠŸç”Ÿæˆ {actual_generated} ä¸ªé¢—ç²’")

        # ç”Ÿæˆé¢—ç²’è¯¦ç»†ä¿¡æ¯
        for i, center in enumerate(centers):
            n_faces = face_distribution[i % len(face_distribution)]
            radius = (self.particle_diameter / 2) * np.random.uniform(0.9, 1.1)

            # ç”Ÿæˆå¤šé¢ä½“
            vertices = PolyhedronGenerator.generate_polyhedron(n_faces, radius)

            # éšæœºæ—‹è½¬
            angles = np.random.uniform(0, 2 * np.pi, 3)
            Rx = self._rotation_matrix_x(angles[0])
            Ry = self._rotation_matrix_y(angles[1])
            Rz = self._rotation_matrix_z(angles[2])
            R = Rz @ Ry @ Rx

            vertices = vertices @ R.T + center

            self.particles.append({
                'center': center,
                'vertices': vertices,
                'n_faces': n_faces,
                'radius': radius
            })

        # ç»Ÿè®¡é¢—ç²’é—´è·
        if len(centers) > 1:
            distances = []
            sample_size = min(100, len(centers))
            sample_indices = np.random.choice(len(centers), sample_size, replace=False)

            for i in sample_indices:
                dist, _ = kdtree.query(centers[i], k=2)
                distances.append(dist[1])  # æœ€è¿‘é‚»è·ç¦»

            print(f"é¢—ç²’é—´è·ç»Ÿè®¡: æœ€å°={np.min(distances):.2f}, "
                  f"å¹³å‡={np.mean(distances):.2f}, æœ€å¤§={np.max(distances):.2f} ä½“ç´ ")

    def _get_face_probability_distribution(self):
        """è·å–é¢æ•°çš„æ¦‚ç‡åˆ†å¸ƒï¼ˆåœ°è´¨ç»Ÿè®¡å­¦ç›¸ä¼¼æ€§ï¼‰"""
        # å¯¹æ•°æ­£æ€åˆ†å¸ƒï¼šæ›´å¤šç®€å•å¤šé¢ä½“ï¼Œå°‘é‡å¤æ‚å¤šé¢ä½“
        faces = np.arange(4, 101)
        mu = np.log(12)  # ä¸­å€¼çº¦ä¸º12é¢
        sigma = 0.8

        prob = np.exp(-(np.log(faces) - mu) ** 2 / (2 * sigma ** 2))
        prob = prob / prob.sum()

        return prob

    def _rotation_matrix_x(self, theta):
        """ç»•Xè½´æ—‹è½¬çŸ©é˜µ"""
        return np.array([
            [1, 0, 0],
            [0, np.cos(theta), -np.sin(theta)],
            [0, np.sin(theta), np.cos(theta)]
        ])

    def _rotation_matrix_y(self, theta):
        """ç»•Yè½´æ—‹è½¬çŸ©é˜µ"""
        return np.array([
            [np.cos(theta), 0, np.sin(theta)],
            [0, 1, 0],
            [-np.sin(theta), 0, np.cos(theta)]
        ])

    def _rotation_matrix_z(self, theta):
        """ç»•Zè½´æ—‹è½¬çŸ©é˜µ"""
        return np.array([
            [np.cos(theta), -np.sin(theta), 0],
            [np.sin(theta), np.cos(theta), 0],
            [0, 0, 1]
        ])

    def create_volume_with_blurred_edges(self):
        """ä½¿ç”¨æ­£æ€åˆ†å¸ƒæ¨¡ç³Šè¾¹ç¼˜åˆ›å»ºä½“ç§¯"""
        print("åˆ›å»ºä¸‰ç»´ä½“ç§¯ï¼ˆæ¨¡ç³Šè¾¹ç¼˜ï¼‰...")

        # åˆå§‹åŒ–ä¸ºç©ºï¼ˆå­”éš™ï¼‰
        self.volume = np.zeros((self.vol_size, self.vol_size, self.vol_size), dtype=np.float32)

        # å¯¹äºæä½å­”éš™ç‡ï¼Œé‡‡ç”¨"åå‘æ€ç»´"ï¼šå…ˆå¡«æ»¡ï¼Œå†æŒ–å­”
        if self.target_porosity < 0.01:
            print("æä½å­”éš™ç‡æ¨¡å¼ï¼šä½¿ç”¨è‡´å¯†å¡«å……ç­–ç•¥")
            # å…ˆå…¨éƒ¨å¡«å……ä¸ºå›ºä½“
            self.volume[:] = 1.0

            # åœ¨é¢—ç²’è¾¹ç•Œå¤„åˆ›å»ºç»†å¾®ç¼éš™
            gap_probability = self.target_porosity * 10  # æ”¾å¤§æ¦‚ç‡ä»¥è¡¥å¿

            for idx, particle in enumerate(tqdm(self.particles, desc="åˆ›å»ºé¢—ç²’é—´ç¼éš™")):
                try:
                    vertices = particle['vertices']
                    hull = ConvexHull(vertices)

                    # è®¡ç®—è¾¹ç•Œæ¡†ï¼ˆæ‰©å¤§ä»¥åŒ…å«ç¼éš™åŒºåŸŸï¼‰
                    min_bounds = np.floor(vertices.min(axis=0) - self.blur_mean).astype(int)
                    max_bounds = np.ceil(vertices.max(axis=0) + self.blur_mean).astype(int)

                    min_bounds = np.clip(min_bounds, 0, self.vol_size - 1)
                    max_bounds = np.clip(max_bounds, 0, self.vol_size)

                    # åªåœ¨è¾¹ç•Œå£³å±‚åˆ›å»ºç¼éš™
                    z_range = range(min_bounds[2], max_bounds[2])
                    y_range = range(min_bounds[1], max_bounds[1])
                    x_range = range(min_bounds[0], max_bounds[0])

                    if len(z_range) == 0 or len(y_range) == 0 or len(x_range) == 0:
                        continue

                    zz, yy, xx = np.meshgrid(z_range, y_range, x_range, indexing='ij')
                    local_grid = np.column_stack([xx.ravel(), yy.ravel(), zz.ravel()])

                    # è®¡ç®—åˆ°é¢—ç²’è¡¨é¢çš„è·ç¦»
                    equations = hull.equations
                    inside_values = local_grid @ equations[:, :3].T + equations[:, 3]
                    inside = np.all(inside_values <= 1e-10, axis=1)

                    # æ‰¾åˆ°è¡¨é¢é™„è¿‘çš„ç‚¹ï¼ˆè¾¹ç•Œå±‚ï¼‰
                    min_dist_to_surface = np.abs(inside_values).min(axis=1)

                    for i, (x, y, z) in enumerate(local_grid):
                        # åªåœ¨é¢—ç²’è¡¨é¢é™„è¿‘ï¼ˆè·ç¦» < blur_meanï¼‰åˆ›å»ºç¼éš™
                        dist = min_dist_to_surface[i]

                        if dist < self.blur_mean:
                            # ä½¿ç”¨æ­£æ€åˆ†å¸ƒæ§åˆ¶ç¼éš™ç”Ÿæˆæ¦‚ç‡
                            gap_prob = np.exp(-(dist - self.blur_mean) ** 2 / (2 * self.blur_std ** 2))

                            # éšæœºå†³å®šæ˜¯å¦åˆ›å»ºç¼éš™
                            if np.random.random() < gap_prob * gap_probability:
                                self.volume[z, y, x] = 0  # åˆ›å»ºç¼éš™

                except Exception as e:
                    continue

        else:
            # å¸¸è§„å­”éš™ç‡ï¼šåŸæœ‰ç®—æ³•
            for idx, particle in enumerate(tqdm(self.particles, desc="å¡«å……é¢—ç²’")):
                try:
                    vertices = particle['vertices']
                    hull = ConvexHull(vertices)

                    # è®¡ç®—è¾¹ç•Œæ¡†
                    min_bounds = np.floor(vertices.min(axis=0)).astype(int)
                    max_bounds = np.ceil(vertices.max(axis=0)).astype(int)

                    min_bounds = np.clip(min_bounds - 10, 0, self.vol_size - 1)
                    max_bounds = np.clip(max_bounds + 10, 0, self.vol_size)

                    # åœ¨è¾¹ç•Œæ¡†å†…æ£€æŸ¥ç‚¹
                    z_range = range(min_bounds[2], max_bounds[2])
                    y_range = range(min_bounds[1], max_bounds[1])
                    x_range = range(min_bounds[0], max_bounds[0])

                    if len(z_range) == 0 or len(y_range) == 0 or len(x_range) == 0:
                        continue

                    zz, yy, xx = np.meshgrid(z_range, y_range, x_range, indexing='ij')
                    local_grid = np.column_stack([xx.ravel(), yy.ravel(), zz.ravel()])

                    # åˆ¤æ–­ç‚¹æ˜¯å¦åœ¨å‡¸åŒ…å†…
                    equations = hull.equations
                    inside = np.all(local_grid @ equations[:, :3].T + equations[:, 3] <= 1e-10, axis=1)

                    # è®¡ç®—åˆ°è¡¨é¢çš„è·ç¦»ï¼ˆç”¨äºæ¨¡ç³Šè¾¹ç¼˜ï¼‰
                    for i, (x, y, z) in enumerate(local_grid):
                        if inside[i]:
                            # è®¡ç®—åˆ°æœ€è¿‘è¡¨é¢çš„è·ç¦»
                            distances = np.abs(vertices @ equations[:, :3].T + equations[:, 3])
                            min_dist = distances.min()

                            # ä½¿ç”¨æ­£æ€åˆ†å¸ƒæ§åˆ¶è¾¹ç¼˜æ¨¡ç³Š
                            blur_value = np.exp(-(min_dist - self.blur_mean) ** 2 / (2 * self.blur_std ** 2))

                            # å åŠ åˆ°ä½“ç§¯
                            self.volume[z, y, x] = max(self.volume[z, y, x], 1.0 - blur_value)

                except Exception as e:
                    continue

            # åº”ç”¨æ•´ä½“é«˜æ–¯æ¨¡ç³Šï¼ˆæ¨¡æ‹Ÿè¾¹ç¼˜æ¨¡ç³Šæ•ˆæœï¼‰
            self.volume = gaussian_filter(self.volume, sigma=self.blur_std / 2)

        # äºŒå€¼åŒ–ï¼šé˜ˆå€¼è®¾å®š
        threshold = 0.5
        self.gap_mask = (self.volume < threshold).astype(np.uint8)

        actual_porosity = self.gap_mask.sum() / self.gap_mask.size
        print(f"\nå®é™…å­”éš™ç‡: {actual_porosity:.6f}")
        print(f"ç›®æ ‡å­”éš™ç‡: {self.target_porosity:.6f}")
        print(f"åå·®: {abs(actual_porosity - self.target_porosity):.6f}")

        # å¦‚æœåå·®å¤ªå¤§ï¼Œå°è¯•è°ƒæ•´é˜ˆå€¼
        if abs(actual_porosity - self.target_porosity) > self.target_porosity * 2:
            print("\nå°è¯•é€šè¿‡è°ƒæ•´é˜ˆå€¼ä¼˜åŒ–å­”éš™ç‡...")

            # äºŒåˆ†æœç´¢æœ€ä½³é˜ˆå€¼
            best_threshold = threshold
            best_diff = abs(actual_porosity - self.target_porosity)

            for t in np.linspace(0.1, 0.9, 20):
                test_mask = (self.volume < t).astype(np.uint8)
                test_porosity = test_mask.sum() / test_mask.size
                diff = abs(test_porosity - self.target_porosity)

                if diff < best_diff:
                    best_diff = diff
                    best_threshold = t

            if best_threshold != threshold:
                print(f"æ‰¾åˆ°æ›´ä¼˜é˜ˆå€¼: {best_threshold:.3f}")
                self.gap_mask = (self.volume < best_threshold).astype(np.uint8)
                actual_porosity = self.gap_mask.sum() / self.gap_mask.size
                print(f"ä¼˜åŒ–åå­”éš™ç‡: {actual_porosity:.6f}")
                print(f"æ–°åå·®: {abs(actual_porosity - self.target_porosity):.6f}")

    def extract_gap_regions(self):
        """æå–ç¼éš™åŒºåŸŸ"""
        print("æå–ç¼éš™åŒºåŸŸ...")
        self.gap_regions, n_regions = measure.label(
            self.gap_mask, connectivity=3, return_num=True
        )
        print(f"ç‹¬ç«‹ç¼éš™åŒºåŸŸæ•°: {n_regions}")

        return n_regions

    def find_widest_path(self):
        """å¯»æ‰¾æœ€å®½ç¼éš™è·¯å¾„"""
        print("è®¡ç®—ç¼éš™å®½åº¦...")
        from scipy.ndimage import distance_transform_edt

        distance = distance_transform_edt(self.gap_mask)
        gap_width = distance * 2

        print("å¯»æ‰¾æœ€å®½è·¯å¾„...")
        # ä¿®å¤ï¼šä½¿ç”¨ skeletonize è€Œä¸æ˜¯ skeletonize_3d
        skeleton = morphology.skeletonize(self.gap_mask)
        skeleton_coords = np.argwhere(skeleton == 1)

        if len(skeleton_coords) == 0:
            print("âš ï¸  è­¦å‘Š: æ— æœ‰æ•ˆè·¯å¾„ï¼ˆéª¨æ¶ä¸ºç©ºï¼‰")
            self.widest_path = []
            return None, 0

        start = np.unravel_index(np.argmax(gap_width), gap_width.shape)

        visited = np.zeros_like(self.gap_mask, dtype=bool)
        queue = deque([(start[0], start[1], start[2], [start], [gap_width[start]])])
        max_avg = 0
        best_path = []

        max_iterations = 5000
        iteration = 0

        while queue and iteration < max_iterations:
            z, y, x, path, widths = queue.popleft()
            iteration += 1

            if visited[z, y, x]:
                continue
            visited[z, y, x] = True

            avg = np.mean(widths)
            if avg > max_avg and len(path) > 3:
                max_avg = avg
                best_path = path.copy()

            for dz, dy, dx in [(-1, 0, 0), (1, 0, 0), (0, -1, 0), (0, 1, 0), (0, 0, -1), (0, 0, 1)]:
                nz, ny, nx = z + dz, y + dy, x + dx
                if (0 <= nz < self.vol_size and 0 <= ny < self.vol_size and
                        0 <= nx < self.vol_size and skeleton[nz, ny, nx] == 1 and
                        not visited[nz, ny, nx]):
                    queue.append((nz, ny, nx, path + [(nz, ny, nx)],
                                  widths + [gap_width[nz, ny, nx]]))

        self.widest_path = best_path

        if len(best_path) > 0:
            print(f"âœ“ æœ€å®½è·¯å¾„å¹³å‡å®½åº¦: {max_avg:.2f} ä½“ç´ ")
            print(f"  è·¯å¾„é•¿åº¦: {len(best_path)} ä¸ªç‚¹")
        else:
            print("âš ï¸  è­¦å‘Š: æœªæ‰¾åˆ°æœ‰æ•ˆè·¯å¾„")

        return best_path, max_avg

    def visualize_results(self):
        """å¯è§†åŒ–ç»“æœ"""
        print("ç”Ÿæˆå¯è§†åŒ–...")

        # 1. é¢—ç²’ä¸‰ç»´åˆ†å¸ƒï¼ˆPlotlyï¼‰
        self._plot_particles_3d()

        # 2. ç¼éš™åˆ‡ç‰‡å¯è§†åŒ–
        self._plot_gap_slices()

        # 3. æœ€å®½è·¯å¾„å¯è§†åŒ–
        self._plot_widest_path()

        # 4. ç»Ÿè®¡åˆ†æ
        self._plot_statistics()

    def _plot_particles_3d(self):
        """ç»˜åˆ¶é¢—ç²’ä¸‰ç»´åˆ†å¸ƒï¼ˆæ”¹è¿›ç‰ˆï¼šå¤šç§å¯è§†åŒ–æ–¹å¼ï¼‰"""
        print("ç”Ÿæˆé¢—ç²’ä¸‰ç»´å¯è§†åŒ–...")

        # æ–¹æ³•1ï¼šPlotlyäº¤äº’å¼3Dæ•£ç‚¹å›¾ + å‡¸åŒ…è¡¨é¢
        fig = go.Figure()

        # é‡‡æ ·æ˜¾ç¤ºï¼ˆé¿å…è¿‡å¤šï¼‰
        sample_size = min(100, len(self.particles))
        sampled_indices = np.random.choice(len(self.particles), sample_size, replace=False)

        # æ”¶é›†æ‰€æœ‰é¢—ç²’ä¸­å¿ƒç”¨äºæ•£ç‚¹å›¾
        centers = np.array([p['center'] for p in self.particles])
        radii = np.array([p['radius'] for p in self.particles])
        face_counts = np.array([p['n_faces'] for p in self.particles])

        # 1. ç»˜åˆ¶é¢—ç²’ä¸­å¿ƒï¼ˆæŒ‰é¢æ•°ç€è‰²ï¼‰
        fig.add_trace(go.Scatter3d(
            x=centers[:, 0],
            y=centers[:, 1],
            z=centers[:, 2],
            mode='markers',
            marker=dict(
                size=radii / 2,  # æŒ‰åŠå¾„ç¼©æ”¾
                color=face_counts,
                colorscale='Viridis',
                colorbar=dict(title="é¢æ•°"),
                opacity=0.8,
                line=dict(color='white', width=0.5)
            ),
            name='é¢—ç²’ä¸­å¿ƒ',
            text=[f'é¢æ•°:{fc}, åŠå¾„:{r:.2f}' for fc, r in zip(face_counts, radii)],
            hoverinfo='text'
        ))

        # 2. ç»˜åˆ¶éƒ¨åˆ†é¢—ç²’çš„è¡¨é¢ï¼ˆåŠé€æ˜ï¼‰
        colors = ['lightblue', 'lightcoral', 'lightgreen', 'lightyellow', 'lightpink']

        for i, idx in enumerate(sampled_indices[:20]):  # åªæ˜¾ç¤º20ä¸ªè¯¦ç»†è¡¨é¢
            particle = self.particles[idx]
            vertices = particle['vertices']

            try:
                hull = ConvexHull(vertices)

                # æå–è¡¨é¢ä¸‰è§’å½¢
                x, y, z = vertices[:, 0], vertices[:, 1], vertices[:, 2]
                i_faces, j_faces, k_faces = hull.simplices.T

                fig.add_trace(go.Mesh3d(
                    x=x, y=y, z=z,
                    i=i_faces, j=j_faces, k=k_faces,
                    opacity=0.3,
                    color=colors[i % len(colors)],
                    flatshading=False,
                    showlegend=False,
                    hoverinfo='skip'
                ))
            except:
                continue

        fig.update_layout(
            title=dict(
                text=f"é¢—ç²’ä¸‰ç»´åˆ†å¸ƒ (æ€»æ•°={len(self.particles)}, æ˜¾ç¤º={sample_size})<br>"
                     f"å­”éš™ç‡={self.gap_mask.sum() / self.gap_mask.size:.6f}",
                x=0.5,
                xanchor='center'
            ),
            scene=dict(
                xaxis_title='X (ä½“ç´ )',
                yaxis_title='Y (ä½“ç´ )',
                zaxis_title='Z (ä½“ç´ )',
                aspectmode='cube',
                camera=dict(
                    eye=dict(x=1.5, y=1.5, z=1.5)
                )
            ),
            width=1000,
            height=900,
            showlegend=True
        )

        fig.write_html(os.path.join(self.save_dir, "particles_3d_interactive.html"))
        print("âœ“ å·²ä¿å­˜: particles_3d_interactive.html (äº¤äº’å¼3D)")

        # æ–¹æ³•2ï¼šåˆ‡ç‰‡æŠ•å½±å›¾ï¼ˆæ˜¾ç¤ºé¢—ç²’å¯†åº¦åˆ†å¸ƒï¼‰
        self._plot_particle_density_slices()

        # æ–¹æ³•3ï¼šç»Ÿè®¡åˆ†æå¯è§†åŒ–
        self._plot_particle_spatial_distribution()

    def _plot_particle_density_slices(self):
        """ç»˜åˆ¶é¢—ç²’å¯†åº¦åˆ‡ç‰‡å›¾"""
        print("ç”Ÿæˆé¢—ç²’å¯†åº¦åˆ‡ç‰‡...")

        # åˆ›å»ºå¯†åº¦åœº
        density = np.zeros((self.vol_size, self.vol_size, self.vol_size), dtype=np.float32)

        for particle in self.particles:
            center = particle['center'].astype(int)
            radius = int(particle['radius'])

            # åœ¨é¢—ç²’ä¸­å¿ƒå‘¨å›´æ ‡è®°
            z_min, z_max = max(0, center[2] - radius), min(self.vol_size, center[2] + radius)
            y_min, y_max = max(0, center[1] - radius), min(self.vol_size, center[1] + radius)
            x_min, x_max = max(0, center[0] - radius), min(self.vol_size, center[0] + radius)

            density[z_min:z_max, y_min:y_max, x_min:x_max] += 1

        # ç»˜åˆ¶åˆ‡ç‰‡
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))

        slices_z = [self.vol_size // 6, self.vol_size // 3, self.vol_size // 2,
                    2 * self.vol_size // 3, 5 * self.vol_size // 6, self.vol_size - 1]

        for ax, z in zip(axes.flat, slices_z):
            im = ax.imshow(density[z, :, :], cmap='hot', origin='lower', interpolation='bilinear')
            ax.set_title(f'Zåˆ‡ç‰‡ = {z} (é¢—ç²’å¯†åº¦)', fontsize=12, fontweight='bold')
            ax.set_xlabel('X (ä½“ç´ )')
            ax.set_ylabel('Y (ä½“ç´ )')

            # å åŠ é¢—ç²’ä¸­å¿ƒç‚¹
            particles_in_slice = [p for p in self.particles
                                  if abs(p['center'][2] - z) < 5]
            if particles_in_slice:
                x_centers = [p['center'][0] for p in particles_in_slice]
                y_centers = [p['center'][1] for p in particles_in_slice]
                ax.scatter(x_centers, y_centers, c='cyan', s=20, marker='x', alpha=0.8)

            plt.colorbar(im, ax=ax, label='å¯†åº¦')

        plt.tight_layout()
        plt.savefig(os.path.join(self.save_dir, "particle_density_slices.png"),
                    dpi=300, bbox_inches='tight')
        plt.close()
        print("âœ“ å·²ä¿å­˜: particle_density_slices.png")

    def _plot_particle_spatial_distribution(self):
        """ç»˜åˆ¶é¢—ç²’ç©ºé—´åˆ†å¸ƒç»Ÿè®¡"""
        print("ç”Ÿæˆç©ºé—´åˆ†å¸ƒç»Ÿè®¡å›¾...")

        centers = np.array([p['center'] for p in self.particles])

        fig = plt.figure(figsize=(16, 12))

        # 1. XYå¹³é¢æŠ•å½±
        ax1 = plt.subplot(2, 3, 1)
        ax1.scatter(centers[:, 0], centers[:, 1], alpha=0.5, s=10, c='blue')
        ax1.set_xlabel('X (ä½“ç´ )')
        ax1.set_ylabel('Y (ä½“ç´ )')
        ax1.set_title('XYå¹³é¢æŠ•å½±')
        ax1.grid(True, alpha=0.3)
        ax1.set_aspect('equal')

        # 2. XZå¹³é¢æŠ•å½±
        ax2 = plt.subplot(2, 3, 2)
        ax2.scatter(centers[:, 0], centers[:, 2], alpha=0.5, s=10, c='green')
        ax2.set_xlabel('X (ä½“ç´ )')
        ax2.set_ylabel('Z (ä½“ç´ )')
        ax2.set_title('XZå¹³é¢æŠ•å½±')
        ax2.grid(True, alpha=0.3)
        ax2.set_aspect('equal')

        # 3. YZå¹³é¢æŠ•å½±
        ax3 = plt.subplot(2, 3, 3)
        ax3.scatter(centers[:, 1], centers[:, 2], alpha=0.5, s=10, c='red')
        ax3.set_xlabel('Y (ä½“ç´ )')
        ax3.set_ylabel('Z (ä½“ç´ )')
        ax3.set_title('YZå¹³é¢æŠ•å½±')
        ax3.grid(True, alpha=0.3)
        ax3.set_aspect('equal')

        # 4. Xæ–¹å‘å¯†åº¦åˆ†å¸ƒ
        ax4 = plt.subplot(2, 3, 4)
        ax4.hist(centers[:, 0], bins=30, color='steelblue', edgecolor='black', alpha=0.7)
        ax4.set_xlabel('X (ä½“ç´ )')
        ax4.set_ylabel('é¢—ç²’æ•°é‡')
        ax4.set_title('Xæ–¹å‘åˆ†å¸ƒ')
        ax4.grid(True, alpha=0.3)

        # 5. Yæ–¹å‘å¯†åº¦åˆ†å¸ƒ
        ax5 = plt.subplot(2, 3, 5)
        ax5.hist(centers[:, 1], bins=30, color='coral', edgecolor='black', alpha=0.7)
        ax5.set_xlabel('Y (ä½“ç´ )')
        ax5.set_ylabel('é¢—ç²’æ•°é‡')
        ax5.set_title('Yæ–¹å‘åˆ†å¸ƒ')
        ax5.grid(True, alpha=0.3)

        # 6. Zæ–¹å‘å¯†åº¦åˆ†å¸ƒ
        ax6 = plt.subplot(2, 3, 6)
        ax6.hist(centers[:, 2], bins=30, color='mediumseagreen', edgecolor='black', alpha=0.7)
        ax6.set_xlabel('Z (ä½“ç´ )')
        ax6.set_ylabel('é¢—ç²’æ•°é‡')
        ax6.set_title('Zæ–¹å‘åˆ†å¸ƒ')
        ax6.grid(True, alpha=0.3)

        plt.tight_layout()
        plt.savefig(os.path.join(self.save_dir, "particle_spatial_distribution.png"),
                    dpi=300, bbox_inches='tight')
        plt.close()
        print("âœ“ å·²ä¿å­˜: particle_spatial_distribution.png")

    def _plot_gap_slices(self):
        """ç»˜åˆ¶ç¼éš™åˆ‡ç‰‡"""
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))

        slices_z = [self.vol_size // 6, self.vol_size // 3, self.vol_size // 2,
                    2 * self.vol_size // 3, 5 * self.vol_size // 6, self.vol_size - 1]

        for ax, z in zip(axes.flat, slices_z):
            im = ax.imshow(self.gap_regions[z, :, :], cmap='tab20', origin='lower')
            ax.set_title(f'Zåˆ‡ç‰‡ = {z} (ç¼éš™åŒºåŸŸ)', fontsize=12, fontweight='bold')
            ax.set_xlabel('X')
            ax.set_ylabel('Y')
            plt.colorbar(im, ax=ax, label='åŒºåŸŸID')

        plt.tight_layout()
        plt.savefig(os.path.join(self.save_dir, "gap_slices.png"), dpi=300, bbox_inches='tight')
        plt.close()
        print("âœ“ å·²ä¿å­˜: gap_slices.png")

    def _plot_widest_path(self):
        """ç»˜åˆ¶æœ€å®½è·¯å¾„"""
        if not self.widest_path or len(self.widest_path) == 0:
            return

        fig = go.Figure()

        # æ˜¾ç¤ºç¼éš™ç©ºé—´ï¼ˆé‡‡æ ·ï¼‰
        z, y, x = np.where(self.gap_mask > 0)
        sample_indices = np.random.choice(len(x), min(3000, len(x)), replace=False)

        fig.add_trace(go.Scatter3d(
            x=x[sample_indices], y=y[sample_indices], z=z[sample_indices],
            mode='markers',
            marker=dict(size=1, color='lightgray', opacity=0.2),
            name='ç¼éš™ç©ºé—´'
        ))

        # æ˜¾ç¤ºæœ€å®½è·¯å¾„
        path_array = np.array(self.widest_path)
        fig.add_trace(go.Scatter3d(
            x=path_array[:, 2], y=path_array[:, 1], z=path_array[:, 0],
            mode='lines+markers',
            line=dict(color='red', width=6),
            marker=dict(size=4, color='red'),
            name='æœ€å®½ç¼éš™è·¯å¾„'
        ))

        fig.update_layout(
            title="ä¸‰ç»´æœ€å®½ç¼éš™è·¯å¾„",
            scene=dict(
                xaxis_title='X', yaxis_title='Y', zaxis_title='Z',
                aspectmode='cube'
            ),
            width=900, height=900
        )

        fig.write_html(os.path.join(self.save_dir, "widest_path_3d.html"))
        print("âœ“ å·²ä¿å­˜: widest_path_3d.html")

    def _plot_statistics(self):
        """ç»˜åˆ¶ç»Ÿè®¡å›¾"""
        fig, axes = plt.subplots(2, 2, figsize=(14, 10))

        # é¢—ç²’é¢æ•°åˆ†å¸ƒ
        face_counts = [p['n_faces'] for p in self.particles]
        axes[0, 0].hist(face_counts, bins=30, color='steelblue', edgecolor='black')
        axes[0, 0].set_xlabel('å¤šé¢ä½“é¢æ•°')
        axes[0, 0].set_ylabel('é¢‘æ•°')
        axes[0, 0].set_title('é¢—ç²’å¤æ‚åº¦åˆ†å¸ƒ')
        axes[0, 0].grid(True, alpha=0.3)

        # é¢—ç²’åŠå¾„åˆ†å¸ƒ
        radii = [p['radius'] for p in self.particles]
        axes[0, 1].hist(radii, bins=30, color='coral', edgecolor='black')
        axes[0, 1].set_xlabel('é¢—ç²’åŠå¾„ (ä½“ç´ )')
        axes[0, 1].set_ylabel('é¢‘æ•°')
        axes[0, 1].set_title('é¢—ç²’å¤§å°åˆ†å¸ƒ')
        axes[0, 1].grid(True, alpha=0.3)

        # ç¼éš™åŒºåŸŸå¤§å°åˆ†å¸ƒ
        region_sizes = [np.sum(self.gap_regions == i)
                        for i in range(1, self.gap_regions.max() + 1)]
        axes[1, 0].hist(region_sizes, bins=30, color='mediumseagreen', edgecolor='black')
        axes[1, 0].set_xlabel('ç¼éš™åŒºåŸŸä½“ç§¯ (ä½“ç´ )')
        axes[1, 0].set_ylabel('é¢‘æ•°')
        axes[1, 0].set_title('ç¼éš™åŒºåŸŸå¤§å°åˆ†å¸ƒ')
        axes[1, 0].set_yscale('log')
        axes[1, 0].grid(True, alpha=0.3)

        # å­”éš™ç‡å¯¹æ¯”
        actual_p = self.gap_mask.sum() / self.gap_mask.size
        axes[1, 1].bar(['ç›®æ ‡å­”éš™ç‡', 'å®é™…å­”éš™ç‡'],
                       [self.target_porosity, actual_p],
                       color=['orange', 'green'], edgecolor='black')
        axes[1, 1].set_ylabel('å­”éš™ç‡')
        axes[1, 1].set_title('å­”éš™ç‡å¯¹æ¯”')
        axes[1, 1].grid(True, alpha=0.3, axis='y')

        plt.tight_layout()
        plt.savefig(os.path.join(self.save_dir, "statistics.png"), dpi=300, bbox_inches='tight')
        plt.close()
        print("âœ“ å·²ä¿å­˜: statistics.png")

    def save_data(self):
        """ä¿å­˜æ•°æ®"""
        np.save(os.path.join(self.save_dir, 'volume.npy'), self.volume)
        np.save(os.path.join(self.save_dir, 'gap_mask.npy'), self.gap_mask)
        np.save(os.path.join(self.save_dir, 'gap_regions.npy'), self.gap_regions)

        results = {
            'vol_size': self.vol_size,
            'target_porosity': float(self.target_porosity),
            'actual_porosity': float(self.gap_mask.sum() / self.gap_mask.size),
            'n_particles': len(self.particles),
            'n_gap_regions': int(self.gap_regions.max()),
            'blur_mean': self.blur_mean,
            'blur_std': self.blur_std
        }

        with open(os.path.join(self.save_dir, 'results.json'), 'w') as f:
            json.dump(results, f, indent=4)

        print("âœ“ å·²ä¿å­˜æ‰€æœ‰æ•°æ®")


# ===================== 4. 3D GANç½‘ç»œï¼ˆå†…å­˜ä¼˜åŒ–ç‰ˆï¼‰ =====================
class Generator3D(nn.Module):
    """3Dç”Ÿæˆå™¨ï¼ˆå†…å­˜ä¼˜åŒ–ï¼šä½¿ç”¨æ›´å°çš„ç‰¹å¾ç»´åº¦ï¼‰"""

    def __init__(self, latent_dim=64, vol_size=32):  # å‡å°latent_dimå’Œé»˜è®¤å°ºå¯¸
        super().__init__()
        self.latent_dim = latent_dim

        # ç¼–ç å™¨ï¼ˆå‡å°‘ä¸­é—´å±‚å¤§å°ï¼‰
        self.encoder = nn.Sequential(
            nn.Linear(latent_dim, 256),  # ä»512å‡åˆ°256
            nn.ReLU(True),
            nn.Linear(256, 4 * 4 * 4 * 32)  # ä»64å‡åˆ°32é€šé“
        )

        # ä¸»å¹²ç½‘ç»œï¼ˆå‡å°‘é€šé“æ•°ï¼‰
        self.main = nn.ModuleList([
            # 4 -> 8
            nn.ConvTranspose3d(32, 64, 4, 2, 1),  # ä»128å‡åˆ°64
            nn.BatchNorm3d(64),
            nn.ReLU(True),

            # 8 -> 16
            nn.ConvTranspose3d(64, 32, 4, 2, 1),  # ä»64å‡åˆ°32
            nn.BatchNorm3d(32),
            nn.ReLU(True),

            # 16 -> 32
            nn.ConvTranspose3d(32, 16, 4, 2, 1),  # ä»32å‡åˆ°16
            nn.BatchNorm3d(16),
            nn.ReLU(True),

            # è¾“å‡ºå±‚
            nn.Conv3d(16, 1, 3, 1, 1),
            nn.Sigmoid()
        ])

        # ç®€åŒ–çš„æ³¨æ„åŠ›æ¨¡å—ï¼ˆå¯é€‰ï¼‰
        self.use_attention = vol_size <= 32  # åªåœ¨å°å°ºå¯¸æ—¶ä½¿ç”¨
        if self.use_attention:
            self.attention = SelfAttention3D(32)

    def forward(self, z):
        x = self.encoder(z).view(-1, 32, 4, 4, 4)

        for i, layer in enumerate(self.main):
            x = layer(x)
            # åªåœ¨ç‰¹å®šå±‚ä½¿ç”¨æ³¨æ„åŠ›ï¼Œä¸”å°ºå¯¸ä¸å¤§æ—¶
            if self.use_attention and i == 5 and x.size(2) <= 16:
                x = self.attention(x)

        return x


class Discriminator3D(nn.Module):
    """3Dåˆ¤åˆ«å™¨ï¼ˆå†…å­˜ä¼˜åŒ–ç‰ˆï¼‰"""

    def __init__(self, vol_size=32):
        super().__init__()

        self.main = nn.Sequential(
            # 32 -> 16
            nn.Conv3d(1, 16, 4, 2, 1),  # ä»32å‡åˆ°16
            nn.LeakyReLU(0.2, True),
            nn.Dropout3d(0.2),  # æ·»åŠ dropoutå‡å°‘è¿‡æ‹Ÿåˆ

            # 16 -> 8
            nn.Conv3d(16, 32, 4, 2, 1),  # ä»64å‡åˆ°32
            nn.BatchNorm3d(32),
            nn.LeakyReLU(0.2, True),
            nn.Dropout3d(0.2),

            # 8 -> 4
            nn.Conv3d(32, 64, 4, 2, 1),  # ä»128å‡åˆ°64
            nn.BatchNorm3d(64),
            nn.LeakyReLU(0.2, True),
            nn.Dropout3d(0.2),

            # 4 -> 1
            nn.Conv3d(64, 1, 4, 1, 0),
            nn.Sigmoid()
        )

    def forward(self, x):
        return self.main(x).view(-1, 1)


class SelfAttention3D(nn.Module):
    """3Dè‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼ˆå†…å­˜ä¼˜åŒ–ï¼šå‡å°‘è®¡ç®—é‡ï¼‰"""

    def __init__(self, channels):
        super().__init__()
        # ä½¿ç”¨æ›´å¤§çš„ç¼©å‡æ¯”ä¾‹
        self.query = nn.Conv3d(channels, channels // 16, 1)  # ä»//8æ”¹ä¸º//16
        self.key = nn.Conv3d(channels, channels // 16, 1)
        self.value = nn.Conv3d(channels, channels, 1)
        self.gamma = nn.Parameter(torch.zeros(1))

        # æ·»åŠ æ± åŒ–å±‚å‡å°‘ç©ºé—´ç»´åº¦
        self.pool = nn.AvgPool3d(2, 2)

    def forward(self, x):
        B, C, D, H, W = x.size()

        # å…ˆæ± åŒ–å‡å°‘ç©ºé—´ç»´åº¦
        x_pooled = self.pool(x)
        _, _, D_p, H_p, W_p = x_pooled.size()

        query = self.query(x_pooled).view(B, -1, D_p * H_p * W_p).permute(0, 2, 1)
        key = self.key(x_pooled).view(B, -1, D_p * H_p * W_p)
        value = self.value(x_pooled).view(B, -1, D_p * H_p * W_p)

        attention = torch.softmax(torch.bmm(query, key), dim=-1)
        out = torch.bmm(value, attention.permute(0, 2, 1))
        out = out.view(B, C, D_p, H_p, W_p)

        # ä¸Šé‡‡æ ·å›åŸå§‹å°ºå¯¸
        out = nn.functional.interpolate(out, size=(D, H, W), mode='trilinear', align_corners=False)

        return self.gamma * out + x


# ===================== 5. GANè®­ç»ƒå™¨ï¼ˆå†…å­˜ä¼˜åŒ–ç‰ˆï¼‰ =====================
class PorousGANTrainer:
    """GANè®­ç»ƒå™¨ï¼ˆå†…å­˜ä¼˜åŒ–ï¼‰"""

    def __init__(self, vol_size=32, latent_dim=64, device='cuda', save_dir='output_gan'):
        self.vol_size = vol_size
        self.latent_dim = latent_dim
        self.device = device
        self.save_dir = save_dir

        os.makedirs(save_dir, exist_ok=True)
        os.makedirs(os.path.join(save_dir, 'samples'), exist_ok=True)

        # å†…å­˜ä¼˜åŒ–ï¼šå¦‚æœGPUå†…å­˜ä¸è¶³ï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ°CPU
        try:
            self.G = Generator3D(latent_dim, vol_size).to(device)
            self.D = Discriminator3D(vol_size).to(device)

            # æµ‹è¯•å‰å‘ä¼ æ’­
            test_z = torch.randn(1, latent_dim, device=device)
            _ = self.G(test_z)

            print(f"âœ“ ä½¿ç”¨è®¾å¤‡: {device}")
        except RuntimeError as e:
            if "out of memory" in str(e):
                print(f"âš ï¸  GPUå†…å­˜ä¸è¶³ï¼Œåˆ‡æ¢åˆ°CPUæ¨¡å¼")
                device = 'cpu'
                self.device = device
                torch.cuda.empty_cache()  # æ¸…ç©ºGPUç¼“å­˜

                self.G = Generator3D(latent_dim, vol_size).to(device)
                self.D = Discriminator3D(vol_size).to(device)
            else:
                raise e

        self.opt_G = optim.Adam(self.G.parameters(), lr=0.0001, betas=(0.5, 0.999))  # é™ä½å­¦ä¹ ç‡
        self.opt_D = optim.Adam(self.D.parameters(), lr=0.0001, betas=(0.5, 0.999))

        self.criterion = nn.BCELoss()

        self.history = {'loss_G': [], 'loss_D': []}

    def train(self, real_data_loader, epochs=200, accumulation_steps=4):
        """
        è®­ç»ƒGANï¼ˆæ¢¯åº¦ç´¯ç§¯ä¼˜åŒ–ï¼‰

        Parameters:
        -----------
        accumulation_steps : int
            æ¢¯åº¦ç´¯ç§¯æ­¥æ•°ï¼Œç”¨äºæ¨¡æ‹Ÿæ›´å¤§çš„batch_size
        """
        print(f"å¼€å§‹GANè®­ç»ƒ ({epochs} epochs, è®¾å¤‡: {self.device})")
        if self.device == 'cpu':
            print("ğŸ’¡ æç¤ºï¼šCPUè®­ç»ƒè¾ƒæ…¢ï¼Œå»ºè®®ä½¿ç”¨è¾ƒå°‘çš„epochs")

        for epoch in tqdm(range(epochs), desc="è®­ç»ƒè¿›åº¦"):
            epoch_loss_D = 0
            epoch_loss_G = 0
            n_batches = 0

            for batch_idx, batch_data in enumerate(real_data_loader):
                # è§£åŒ…æ•°æ®
                if isinstance(batch_data, (list, tuple)):
                    real_batch = batch_data[0]
                else:
                    real_batch = batch_data

                batch_size = real_batch.size(0)
                real_batch = real_batch.to(self.device)

                # === è®­ç»ƒåˆ¤åˆ«å™¨ ===
                if batch_idx % accumulation_steps == 0:
                    self.opt_D.zero_grad()

                real_labels = torch.ones(batch_size, 1, device=self.device) * 0.9
                fake_labels = torch.zeros(batch_size, 1, device=self.device) + 0.1

                pred_real = self.D(real_batch)
                loss_D_real = self.criterion(pred_real, real_labels) / accumulation_steps

                z = torch.randn(batch_size, self.latent_dim, device=self.device)
                fake_batch = self.G(z)
                pred_fake = self.D(fake_batch.detach())
                loss_D_fake = self.criterion(pred_fake, fake_labels) / accumulation_steps

                loss_D = (loss_D_real + loss_D_fake)
                loss_D.backward()

                if (batch_idx + 1) % accumulation_steps == 0:
                    self.opt_D.step()

                # === è®­ç»ƒç”Ÿæˆå™¨ ===
                if batch_idx % accumulation_steps == 0:
                    self.opt_G.zero_grad()

                z = torch.randn(batch_size, self.latent_dim, device=self.device)
                fake_batch = self.G(z)
                pred_fake = self.D(fake_batch)
                loss_G = self.criterion(pred_fake, torch.ones(batch_size, 1, device=self.device)) / accumulation_steps

                loss_G.backward()

                if (batch_idx + 1) % accumulation_steps == 0:
                    self.opt_G.step()

                epoch_loss_D += loss_D.item() * accumulation_steps
                epoch_loss_G += loss_G.item() * accumulation_steps
                n_batches += 1

                # å†…å­˜æ¸…ç†
                if self.device == 'cuda' and batch_idx % 10 == 0:
                    torch.cuda.empty_cache()

            avg_loss_D = epoch_loss_D / n_batches
            avg_loss_G = epoch_loss_G / n_batches

            self.history['loss_G'].append(avg_loss_G)
            self.history['loss_D'].append(avg_loss_D)

            if (epoch + 1) % 20 == 0:
                self.save_samples(epoch + 1)
                print(f"\nEpoch {epoch + 1}: Loss_D={avg_loss_D:.4f}, Loss_G={avg_loss_G:.4f}")

        self.save_model()

        # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
        self._plot_training_curves()

    def _plot_training_curves(self):
        """ç»˜åˆ¶è®­ç»ƒæ›²çº¿"""
        plt.figure(figsize=(10, 5))

        plt.plot(self.history['loss_G'], label='Generator Loss', linewidth=2)
        plt.plot(self.history['loss_D'], label='Discriminator Loss', linewidth=2)
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.title('GAN Training Curves')
        plt.legend()
        plt.grid(True, alpha=0.3)

        plt.savefig(os.path.join(self.save_dir, 'training_curves.png'), dpi=300, bbox_inches='tight')
        plt.close()
        print("âœ“ å·²ä¿å­˜è®­ç»ƒæ›²çº¿")

    def save_samples(self, epoch):
        """ä¿å­˜ç”Ÿæˆæ ·æœ¬"""
        self.G.eval()
        with torch.no_grad():
            z = torch.randn(1, self.latent_dim, device=self.device)
            sample = self.G(z).cpu().numpy()[0, 0]

        np.save(os.path.join(self.save_dir, 'samples', f'sample_epoch_{epoch}.npy'), sample)
        self.G.train()

    def save_model(self):
        """ä¿å­˜æ¨¡å‹"""
        torch.save({
            'generator': self.G.state_dict(),
            'discriminator': self.D.state_dict(),
            'history': self.history
        }, os.path.join(self.save_dir, 'model.pth'))
        print("âœ“ å·²ä¿å­˜æ¨¡å‹")

    def generate_batch(self, n_samples=50):
        """ç”Ÿæˆæ‰¹é‡æ ·æœ¬ï¼ˆæ‰¹é‡å¤„ç†ä»¥èŠ‚çœå†…å­˜ï¼‰"""
        self.G.eval()
        samples = []

        # æ‰¹é‡ç”Ÿæˆï¼ˆæ¯æ¬¡ç”Ÿæˆå°‘é‡ä»¥èŠ‚çœå†…å­˜ï¼‰
        batch_size = 5 if self.device == 'cuda' else 1
        n_batches = (n_samples + batch_size - 1) // batch_size

        with torch.no_grad():
            for i in tqdm(range(n_batches), desc="ç”Ÿæˆæ ·æœ¬"):
                current_batch_size = min(batch_size, n_samples - i * batch_size)
                z = torch.randn(current_batch_size, self.latent_dim, device=self.device)
                batch_samples = self.G(z).cpu().numpy()

                for j in range(current_batch_size):
                    sample = batch_samples[j, 0]
                    samples.append(sample)

                    sample_idx = i * batch_size + j
                    np.save(os.path.join(self.save_dir, 'samples',
                                         f'generated_{sample_idx:03d}.npy'), sample)

                # æ¸…ç†å†…å­˜
                if self.device == 'cuda':
                    torch.cuda.empty_cache()

        return samples


# ===================== 6. å¯¹æ¯”åˆ†æ =====================
def compare_methods(blurred_data, gan_samples, save_dir='comparison'):
    """å¯¹æ¯”æ¨¡ç³Šè¾¹ç¼˜æ³•å’ŒGANç”Ÿæˆç»“æœ"""
    os.makedirs(save_dir, exist_ok=True)

    print("=" * 80)
    print("å¯¹æ¯”åˆ†æ: æ¨¡ç³Šè¾¹ç¼˜æ³• vs GANç”Ÿæˆ")
    print("=" * 80)

    # 1. å­”éš™ç‡å¯¹æ¯”
    blurred_porosity = blurred_data.sum() / blurred_data.size
    gan_porosities = [s.sum() / s.size for s in gan_samples]

    print(f"\nå­”éš™ç‡ç»Ÿè®¡:")
    print(f"  æ¨¡ç³Šè¾¹ç¼˜æ³•: {blurred_porosity:.6f}")
    print(f"  GANå¹³å‡: {np.mean(gan_porosities):.6f} Â± {np.std(gan_porosities):.6f}")

    # 2. ç¼éš™è¿é€šæ€§åˆ†æ
    blurred_connectivity = analyze_connectivity(blurred_data)
    gan_connectivities = [analyze_connectivity(s > 0.5) for s in gan_samples]

    print(f"\nè¿é€šæ€§ç»Ÿè®¡:")
    print(f"  æ¨¡ç³Šè¾¹ç¼˜æ³•: {blurred_connectivity:.3f}")
    print(f"  GANå¹³å‡: {np.mean(gan_connectivities):.3f} Â± {np.std(gan_connectivities):.3f}")

    # 3. å¯è§†åŒ–å¯¹æ¯”
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))

    # æ¨¡ç³Šè¾¹ç¼˜æ³•ä¸­é—´åˆ‡ç‰‡
    mid = blurred_data.shape[0] // 2
    axes[0, 0].imshow(blurred_data[mid], cmap='gray')
    axes[0, 0].set_title(f'æ¨¡ç³Šè¾¹ç¼˜æ³• (å­”éš™ç‡={blurred_porosity:.6f})')

    # GANæ ·æœ¬
    for i in range(5):
        ax = axes.flat[i + 1]
        sample = gan_samples[i]
        mid = sample.shape[0] // 2
        ax.imshow(sample[mid], cmap='gray')
        ax.set_title(f'GANæ ·æœ¬{i + 1} (å­”éš™ç‡={gan_porosities[i]:.6f})')

    plt.tight_layout()
    plt.savefig(os.path.join(save_dir, 'comparison.png'), dpi=300)
    plt.close()

    # 4. ç»Ÿè®¡æ›²çº¿
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))

    axes[0].hist(gan_porosities, bins=20, alpha=0.7, label='GAN', color='blue', edgecolor='black')
    axes[0].axvline(blurred_porosity, color='red', linestyle='--', linewidth=2, label='æ¨¡ç³Šè¾¹ç¼˜æ³•')
    axes[0].set_xlabel('å­”éš™ç‡')
    axes[0].set_ylabel('é¢‘æ•°')
    axes[0].set_title('å­”éš™ç‡åˆ†å¸ƒå¯¹æ¯”')
    axes[0].legend()
    axes[0].grid(True, alpha=0.3)

    axes[1].hist(gan_connectivities, bins=20, alpha=0.7, label='GAN', color='green', edgecolor='black')
    axes[1].axvline(blurred_connectivity, color='red', linestyle='--', linewidth=2, label='æ¨¡ç³Šè¾¹ç¼˜æ³•')
    axes[1].set_xlabel('è¿é€šæ€§ç³»æ•°')
    axes[1].set_ylabel('é¢‘æ•°')
    axes[1].set_title('è¿é€šæ€§åˆ†å¸ƒå¯¹æ¯”')
    axes[1].legend()
    axes[1].grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig(os.path.join(save_dir, 'statistics_comparison.png'), dpi=300)
    plt.close()

    print("\nâœ“ å¯¹æ¯”åˆ†æå®Œæˆ")


def analyze_connectivity(volume):
    """åˆ†æè¿é€šæ€§"""
    labeled, n_components = measure.label(volume > 0.5, connectivity=3, return_num=True)

    if n_components == 0:
        return 0.0

    # æœ€å¤§è¿é€šåˆ†é‡å æ¯”
    largest_component_size = max([np.sum(labeled == i) for i in range(1, n_components + 1)])
    total_pore_voxels = np.sum(volume > 0.5)

    if total_pore_voxels == 0:
        return 0.0

    return largest_component_size / total_pore_voxels


# ===================== 7. ä¸»ç¨‹åº =====================
if __name__ == "__main__":
    # ç¬¬1æ­¥ï¼šä½“ç´ å®¹é‡åˆ†æ
    print("ç¬¬1æ­¥ï¼šç†è®ºåˆ†æ")
    analyzer = VoxelCapacityAnalyzer()
    capacity_results = analyzer.analyze_capacity([256, 512])

    # ç¬¬2æ­¥ï¼šæ¨¡ç³Šè¾¹ç¼˜æ³•ç”Ÿæˆ
    print("\nç¬¬2æ­¥ï¼šæ¨¡ç³Šè¾¹ç¼˜æ³•ç”Ÿæˆ")

    # é’ˆå¯¹æä½å­”éš™ç‡ä¼˜åŒ–çš„å‚æ•°é…ç½®
    print("\nğŸ’¡ æä½å­”éš™ç‡(0.0003)ç‰¹æ®Šé…ç½®ï¼š")
    print("  ç­–ç•¥ï¼šè‡´å¯†å †ç§¯ + é¢—ç²’è¾¹ç•Œç¼éš™")
    print("  - å¤§é‡å°é¢—ç²’å¡«å……ç©ºé—´")
    print("  - ä»…åœ¨é¢—ç²’æ¥è§¦é¢äº§ç”Ÿå¾®å°ç¼éš™")
    print("  - ä½¿ç”¨æ¦‚ç‡æ§åˆ¶ç¼éš™ç”Ÿæˆ")
    print()

    blurred_method = BlurredEdgeMethod(
        vol_size=128,  # è¶³å¤Ÿå¤§çš„ç©ºé—´
        target_porosity=0.0003,
        particle_diameter=5,  # å°é¢—ç²’ï¼Œé«˜å¯†åº¦
        compactness=0.99,  # æé«˜å¯†å®åº¦
        blur_mean=1.0,  # å°è¾¹ç¼˜æ¨¡ç³Šï¼ˆç¼éš™å®½åº¦ï¼‰
        blur_std=0.5,  # å°æ–¹å·®
        save_dir='output_blurred'
    )

    blurred_method.generate_particles()
    blurred_method.create_volume_with_blurred_edges()

    # æ£€æŸ¥å­”éš™ç‡åå·®
    actual_porosity = blurred_method.gap_mask.sum() / blurred_method.gap_mask.size
    relative_error = abs(actual_porosity - 0.0003) / 0.0003

    if relative_error > 0.5:  # ç›¸å¯¹è¯¯å·®>50%
        print("\n" + "=" * 80)
        print("âš ï¸  å­”éš™ç‡åå·®åˆ†æ")
        print("=" * 80)
        print(f"ç›®æ ‡å­”éš™ç‡: {0.0003:.6f}")
        print(f"å®é™…å­”éš™ç‡: {actual_porosity:.6f}")
        print(f"ç»å¯¹åå·®: {abs(actual_porosity - 0.0003):.6f}")
        print(f"ç›¸å¯¹è¯¯å·®: {relative_error * 100:.1f}%")

        print("\nåŸå› åˆ†æ:")
        print("  1. æä½å­”éš™ç‡(0.03%)æ¥è¿‘ææ–™ç‰©ç†æé™")
        print("  2. é¢—ç²’é—´å¿…ç„¶å­˜åœ¨å‡ ä½•é—´éš™ï¼ˆä¸å¯å®Œå…¨æ¶ˆé™¤ï¼‰")
        print("  3. è¾¹ç¼˜æ¨¡ç³Šç®—æ³•ä¼šåœ¨é¢—ç²’è¾¹ç•Œäº§ç”Ÿè¿‡æ¸¡åŒº")

        print("\næ”¹è¿›å»ºè®®:")
        if actual_porosity > 0.0003:
            suggestions = [
                ("å¢åŠ é¢—ç²’æ•°é‡",
                 f"å½“å‰: {len(blurred_method.particles)}, å»ºè®®: {int(len(blurred_method.particles) * 1.5)}"),
                ("å‡å°é¢—ç²’ç›´å¾„", f"å½“å‰: 5ä½“ç´ , å»ºè®®: 4ä½“ç´ "),
                ("æé«˜å¯†å®åº¦", f"å½“å‰: 0.99, å»ºè®®: 0.995"),
                ("å‡å°ç¼éš™å®½åº¦", f"å½“å‰blur_mean: 2, å»ºè®®: 1.5"),
                ("å¢å¤§ä½“ç´ ç©ºé—´", f"å½“å‰: 128Â³, å»ºè®®: 256Â³")
            ]
        else:
            suggestions = [
                ("å‡å°‘é¢—ç²’æ•°é‡", f"å½“å‰: {len(blurred_method.particles)}"),
                ("å¢å¤§é¢—ç²’ç›´å¾„", f"å½“å‰: 5ä½“ç´ , å»ºè®®: 6ä½“ç´ "),
                ("é™ä½å¯†å®åº¦", f"å½“å‰: 0.99, å»ºè®®: 0.95")
            ]

        for i, (action, detail) in enumerate(suggestions, 1):
            print(f"  {i}. {action}: {detail}")

        print("\nğŸ’¡ æç¤ºï¼šå¯¹äºå¦‚æ­¤æç«¯çš„ä½å­”éš™ç‡ï¼Œå»ºè®®ä½¿ç”¨æ›´å¤§çš„ä½“ç´ ç©ºé—´")
        print("   (å¦‚256Â³æˆ–512Â³)ä»¥è·å¾—æ›´ç²¾ç¡®çš„æ§åˆ¶ã€‚")
        print("=" * 80 + "\n")
    else:
        print(f"\nâœ“ å­”éš™ç‡æ§åˆ¶è‰¯å¥½ï¼Œç›¸å¯¹è¯¯å·®: {relative_error * 100:.1f}%\n")

    blurred_method.extract_gap_regions()
    blurred_method.find_widest_path()
    blurred_method.visualize_results()
    blurred_method.save_data()

    # ç¬¬3æ­¥ï¼šå‡†å¤‡GANè®­ç»ƒæ•°æ®
    print("\nç¬¬3æ­¥ï¼šå‡†å¤‡GANè®­ç»ƒæ•°æ®")
    blurred_volume = blurred_method.volume

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    from torch.utils.data import TensorDataset, DataLoader

    real_data = torch.FloatTensor(blurred_volume).unsqueeze(0).unsqueeze(0)

    # æ•°æ®å¢å¼ºï¼šåˆ›å»ºå¤šä¸ªæ—‹è½¬å’Œç¿»è½¬ç‰ˆæœ¬
    augmented_data = [real_data]

    # 90åº¦æ—‹è½¬ï¼ˆ3ä¸ªæ–¹å‘ï¼Œæ¯ä¸ªæ–¹å‘3æ¬¡æ—‹è½¬ï¼‰
    for axis in [2, 3, 4]:  # Z, Y, Xè½´
        for k in [1, 2, 3]:
            if axis == 2:  # Zè½´
                augmented_data.append(torch.rot90(real_data, k=k, dims=[3, 4]))
            elif axis == 3:  # Yè½´
                augmented_data.append(torch.rot90(real_data, k=k, dims=[2, 4]))
            else:  # Xè½´
                augmented_data.append(torch.rot90(real_data, k=k, dims=[2, 3]))

    # ç¿»è½¬
    augmented_data.append(torch.flip(real_data, dims=[2]))
    augmented_data.append(torch.flip(real_data, dims=[3]))
    augmented_data.append(torch.flip(real_data, dims=[4]))

    print(f"æ•°æ®å¢å¼ºåæ ·æœ¬æ•°: {len(augmented_data)}")

    dataset = TensorDataset(torch.cat(augmented_data))
    dataloader = DataLoader(dataset, batch_size=4, shuffle=True)

    # ç¬¬4æ­¥ï¼šGANè®­ç»ƒï¼ˆå†…å­˜ä¼˜åŒ–ï¼‰
    print("\nç¬¬4æ­¥ï¼šGANè®­ç»ƒï¼ˆå†…å­˜ä¼˜åŒ–ç‰ˆï¼‰")
    device = 'cuda' if torch.cuda.is_available() else 'cpu'

    # æ ¹æ®å¯ç”¨å†…å­˜è‡ªåŠ¨è°ƒæ•´å‚æ•°
    if device == 'cuda':
        # æ£€æµ‹GPUå†…å­˜
        gpu_mem = torch.cuda.get_device_properties(0).total_memory / (1024 ** 3)  # GB
        print(f"GPUå†…å­˜: {gpu_mem:.2f} GB")

        if gpu_mem < 6:
            gan_vol_size = 24  # è¶…å°å°ºå¯¸
            batch_size = 1
            latent_dim = 32
            print("âš ï¸  GPUå†…å­˜è¾ƒå°(<6GB)ï¼Œä½¿ç”¨è¶…å°ç½‘ç»œé…ç½®")
        elif gpu_mem < 10:
            gan_vol_size = 32  # å°å°ºå¯¸
            batch_size = 2
            latent_dim = 64
            print("ğŸ’¡ GPUå†…å­˜ä¸­ç­‰(6-10GB)ï¼Œä½¿ç”¨å°ç½‘ç»œé…ç½®")
        else:
            gan_vol_size = 48  # ä¸­ç­‰å°ºå¯¸
            batch_size = 4
            latent_dim = 64
            print("âœ“ GPUå†…å­˜å……è¶³(>10GB)ï¼Œä½¿ç”¨ä¸­ç­‰ç½‘ç»œé…ç½®")
    else:
        gan_vol_size = 24
        batch_size = 1
        latent_dim = 32
        print("ğŸ’» ä½¿ç”¨CPUè®­ç»ƒï¼ˆè¾ƒæ…¢ï¼‰")

    print(f"\né…ç½®å‚æ•°:")
    print(f"  - GANç”Ÿæˆå°ºå¯¸: {gan_vol_size}Â³")
    print(f"  - Batch size: {batch_size}")
    print(f"  - Latent dim: {latent_dim}")
    print(f"  - è®­ç»ƒè½®æ•°: 50 (å‡å°‘ä»¥èŠ‚çœæ—¶é—´)")

    # ä¸‹é‡‡æ ·çœŸå®æ•°æ®ä»¥åŒ¹é…GANå°ºå¯¸
    from scipy.ndimage import zoom

    scale_factor = gan_vol_size / blurred_method.vol_size
    print(f"  - ä¸‹é‡‡æ ·æ¯”ä¾‹: {scale_factor:.3f}")

    downsampled_volume = zoom(blurred_volume, scale_factor, order=1)

    downsampled_data = torch.FloatTensor(downsampled_volume).unsqueeze(0).unsqueeze(0)

    # ç®€åŒ–çš„æ•°æ®å¢å¼ºï¼ˆå‡å°‘å†…å­˜ä½¿ç”¨ï¼‰
    augmented_downsampled = [downsampled_data]

    # åªåš90åº¦æ—‹è½¬ï¼Œä¸åšæ‰€æœ‰è§’åº¦
    for k in [1, 2, 3]:
        augmented_downsampled.append(torch.rot90(downsampled_data, k=k, dims=[3, 4]))

    # ç¿»è½¬
    augmented_downsampled.append(torch.flip(downsampled_data, dims=[2]))

    print(f"  - å¢å¼ºåæ ·æœ¬æ•°: {len(augmented_downsampled)}")

    dataset_gan = TensorDataset(torch.cat(augmented_downsampled))
    dataloader_gan = DataLoader(dataset_gan, batch_size=batch_size, shuffle=True,
                                pin_memory=False)  # å…³é—­pin_memoryèŠ‚çœå†…å­˜

    gan_trainer = PorousGANTrainer(vol_size=gan_vol_size, latent_dim=latent_dim,
                                   device=device, save_dir='output_gan')

    # ä½¿ç”¨æ›´å°‘çš„epochs
    gan_trainer.train(dataloader_gan, epochs=50, accumulation_steps=2)

    # ç¬¬5æ­¥ï¼šç”Ÿæˆ50å¹…å›¾åƒ
    print("\nç¬¬5æ­¥ï¼šç”Ÿæˆæ‰¹é‡å›¾åƒ")
    gan_samples = gan_trainer.generate_batch(n_samples=50)

    # ç¬¬6æ­¥ï¼šå¯¹æ¯”åˆ†æ
    print("\nç¬¬6æ­¥ï¼šå¯¹æ¯”åˆ†æ")
    # ä¸‹é‡‡æ ·çœŸå®æ•°æ®ç”¨äºå¯¹æ¯”
    downsampled_mask = zoom(blurred_method.gap_mask.astype(float), scale_factor, order=0)
    compare_methods(downsampled_mask, gan_samples, save_dir='comparison')

    print("\n" + "=" * 80)
    print("æ‰€æœ‰æ­¥éª¤å®Œæˆï¼")
    print("=" * 80)
    print("\nğŸ“Š ç»“æœæ€»ç»“:")
    print(f"  1. æ¨¡ç³Šè¾¹ç¼˜æ³•:")
    print(f"     - ä½“ç´ ç©ºé—´: {blurred_method.vol_size}Â³")
    print(f"     - ç”Ÿæˆé¢—ç²’æ•°: {len(blurred_method.particles)}")
    print(f"     - å®é™…å­”éš™ç‡: {actual_porosity:.6f}")
    print(f"     - ç¼éš™åŒºåŸŸæ•°: {blurred_method.gap_regions.max()}")
    print(f"\n  2. GANç”Ÿæˆ:")
    print(f"     - ç”Ÿæˆæ ·æœ¬æ•°: 50")
    print(f"     - è®­ç»ƒè½®æ•°: 100")
    print(f"     - ç”Ÿæˆå°ºå¯¸: {gan_vol_size}Â³")
    print(f"\nğŸ“ ç»“æœä¿å­˜ä½ç½®:")
    print(f"  - output_blurred/: æ¨¡ç³Šè¾¹ç¼˜æ³•ç»“æœ")
    print(f"  - output_gan/: GANè®­ç»ƒç»“æœå’Œç”Ÿæˆæ ·æœ¬")
    print(f"  - comparison/: å¯¹æ¯”åˆ†æç»“æœ")
    print("=" * 80)